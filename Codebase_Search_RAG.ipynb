{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jja4/local_RAG/blob/main/Codebase_Search_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "I7xhZfK_6T2G",
        "outputId": "28f6c668-1b61-4525-a933-bb856e0816d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [ ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "b0a7b3317a6c4efaa8ee8d8f63e878d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "!pip install langchain rank_bm25 pypdf unstructured chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!apt-get install poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y libtesseract-dev\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monR9xmw6p2E"
      },
      "source": [
        "### Load the required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ7NC1yB6jDX"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, TextLoader\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
        "\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWcedXWq7unE"
      },
      "outputs": [],
      "source": [
        "### Load the PDF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1II05VC6vjr"
      },
      "outputs": [],
      "source": [
        "# Try various file types and sources",
        "# file_path = \"./sample_data/ion_temp_cnn.py\"\n",
        "# data_file = TextLoader(file_path)\n",
        "\n",
        "# file_path = \"./sample_data/Abschlussarbeit_0397062.pdf\"\n",
        "# data_file = UnstructuredPDFLoader(file_path)\n",
        "\n",
        "file_path = \"/content/sample_data/codebase_search_rag.py\"\n",
        "data_file = TextLoader(file_path)\n",
        "\n",
        "docs = data_file.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqUCmzkF7xOe",
        "outputId": "67df6cee-0726-4347-fe74-fd3464b1d908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"Codebase_Search_RAG.ipynb\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1h3Op-6iQmhVfYZ-58Q1EEiI1mBIjNZIg\n",
            "\"\"\"\n",
            "\n",
            "!pip install langchain rank_bm25 pypdf unstructured chromadb\n",
            "!pip install unstructured['pdf'] unstructured\n",
            "!apt-get install poppler-utils\n",
            "!apt-get install -y tesseract-ocr\n",
            "!apt-get install -y libtesseract-dev\n",
            "!pip install pytesseract\n",
            "\n",
            "\"\"\"### Load the required Packages\"\"\"\n",
            "\n",
            "from langchain.document_loaders import UnstructuredPDFLoader, TextLoader\n",
            "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
            "from langchain.vectorstores import Chroma\n",
            "\n",
            "from langchain_core.prompts import ChatPromptTemplate\n",
            "from langchain_core.output_parsers import StrOutputParser\n",
            "from langchain_core.runnables import RunnablePassthrough\n",
            "\n",
            "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
            "from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
            "\n",
            "\n",
            "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
            "\n",
            "import os\n",
            "\n",
            "### Load the PDF file\n",
            "\n",
            "# file_path = \"./sample_data/ion_temp_cnn.py\"\n",
            "# data_file = TextLoader(file_path)\n",
            "file_path = \"./sample_data/Abschlussarbeit_0397062.pdf\"\n",
            "data_file = UnstructuredPDFLoader(file_path)\n",
            "\n",
            "docs = data_file.load()\n",
            "\n",
            "print(docs[0].page_content)\n",
            "\n",
            "\"\"\"### Split Documents and Chunking\"\"\"\n",
            "\n",
            "# create chunks\n",
            "splitter = RecursiveCharacterTextSplitter.from_language(\n",
            "        language=Language.PYTHON, chunk_size=880, chunk_overlap=200\n",
            "    )\n",
            "chunks = splitter.split_documents(docs)\n",
            "\n",
            "chunks[0].page_content\n",
            "\n",
            "# Get Embedding Model from HF via API\n",
            "\n",
            "from google.colab import userdata\n",
            "HF_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
            "\n",
            "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
            "    api_key=HF_TOKEN, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
            ")\n",
            "\n",
            "\"\"\"### VectorStore\"\"\"\n",
            "\n",
            "type(chunks)\n",
            "\n",
            "# Vector store with the selected embedding model\n",
            "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
            "\n",
            "vectorstore_retreiver = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
            "\n",
            "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
            "keyword_retriever.k =  5\n",
            "\n",
            "\"\"\"### Ensemble Retriever\"\"\"\n",
            "\n",
            "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
            "                                                   keyword_retriever],\n",
            "                                       weights=[0.5, 0.5])\n",
            "\n",
            "llm = HuggingFaceEndpoint(\n",
            "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "    temperature = 0.3, max_new_tokens = 1024,\n",
            "    huggingfacehub_api_token=HF_TOKEN,\n",
            ")\n",
            "\n",
            "\"\"\"### Prompt Template:\"\"\"\n",
            "\n",
            "template = \"\"\"\n",
            "<|system|>>\n",
            "You are a helpful AI Assistant that follows instructions extremely well.\n",
            "Use the following context to answer user question.\n",
            "\n",
            "Think step by step before answering the question. You will get a $100 tip if you provide correct answer.\n",
            "\n",
            "CONTEXT: {context}\n",
            "</s>\n",
            "<|user|>\n",
            "{query}\n",
            "</s>\n",
            "<|assistant|>\n",
            "\"\"\"\n",
            "\n",
            "prompt = ChatPromptTemplate.from_template(template)\n",
            "output_parser = StrOutputParser()\n",
            "\n",
            "chain = (\n",
            "    {\"context\": ensemble_retriever, \"query\": RunnablePassthrough()}\n",
            "    | prompt\n",
            "    | llm\n",
            "    | output_parser\n",
            ")\n",
            "\n",
            "print(chain.invoke(\"Why is the core temperature in the plasma more difficult to predict?\"))\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZk8Ob1U8DZK"
      },
      "source": [
        "### Split Documents and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZ1j1Ns73mU"
      },
      "outputs": [],
      "source": [
        "# create chunks\n",
        "splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "        language=Language.PYTHON, chunk_size=880, chunk_overlap=200\n",
        "    )\n",
        "chunks = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "3o58WVXM8SvN",
        "outputId": "0c11d46e-6a2a-482e-b73f-19c73fae2f79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# -*- coding: utf-8 -*-\\n\"\"\"Codebase_Search_RAG.ipynb\\n\\nAutomatically generated by Colab.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1h3Op-6iQmhVfYZ-58Q1EEiI1mBIjNZIg\\n\"\"\"\\n\\n!pip install langchain rank_bm25 pypdf unstructured chromadb\\n!pip install unstructured[\\'pdf\\'] unstructured\\n!apt-get install poppler-utils\\n!apt-get install -y tesseract-ocr\\n!apt-get install -y libtesseract-dev\\n!pip install pytesseract\\n\\n\"\"\"### Load the required Packages\"\"\"\\n\\nfrom langchain.document_loaders import UnstructuredPDFLoader, TextLoader\\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch3SR3Wu8Tnw"
      },
      "outputs": [],
      "source": [
        "# Get Embedding Model from HF via API\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key=HF_TOKEN, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfB7uezt9Vud"
      },
      "source": [
        "### VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ38mmimAW-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a4e6a3-4f58-406c-91a7-628fe1f7aecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "type(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeIka0wR87kF"
      },
      "outputs": [],
      "source": [
        "# Vector store with the selected embedding model\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaaxtuWw9uJM"
      },
      "outputs": [],
      "source": [
        "vectorstore_retreiver = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMucCA4z9VAn"
      },
      "outputs": [],
      "source": [
        "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
        "keyword_retriever.k =  5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UCqmV5-D-r"
      },
      "source": [
        "### Ensemble Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVhLxoWh95dv"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
        "                                                   keyword_retriever],\n",
        "                                       weights=[0.5, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG1h9tvE-KqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b63262-01e0-4d35-fad3-6c2adc9ede33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    temperature = 0.3, max_new_tokens = 1024,\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-bQqY8-kUH"
      },
      "source": [
        "### Prompt Template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C8Vn8PR-QJo"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "<|system|>>\n",
        "You are a helpful AI Assistant that follows instructions extremely well.\n",
        "Use the following context to answer user question.\n",
        "\n",
        "Think step by step before answering the question. You will get a $100 tip if you provide correct answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eDFcNd_Kru"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JtvKJk2_Ok2"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": ensemble_retriever, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCwFiDHm3dQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87caffa-aa35-469f-9b58-31ad4efd9135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s>\n",
            "```\n",
            "The Vector Store `vectorstore_retreiver` and `keyword_retreiver` are two different retrievers used in the codebase for searching and retrieving documents.\n",
            "\n",
            "`vectorstore_retreiver` is an instance of the `Chroma` class, which is a vector store that uses the `sentence-transformers/all-MiniLM-L6-v2` model to embed the documents. The `Chroma` class is a type of vector store that allows for efficient querying and retrieval of documents based on their embeddings.\n",
            "\n",
            "When you query the `vectorstore_retreiver` with a query, it uses the `sentence-transformers/all-MiniLM-L6-v2` model to embed the query and then searches for the most similar documents in the vector store. The results are then returned as a list of documents.\n",
            "\n",
            "`keyword_retreiver`, on the other hand, is an instance of the `BM25Retriever` class, which is a type of retriever that uses the BM25 algorithm to rank documents based on their relevance to the query. The `BM25Retriever` class is a type of retriever that allows for efficient querying and retrieval of documents based on their keywords.\n",
            "\n",
            "When you query the `keyword_retreiver` with a query, it uses the BM25 algorithm to rank the documents in the corpus based on their relevance to the query. The results are then returned as a list of documents.\n",
            "\n",
            "In the code, the `vectorstore_retreiver` and `keyword_retreiver` are used together in an ensemble retriever, which combines the results from both retrievers to produce a final ranking of the documents. This allows for a more robust and accurate search result.\n",
            "\n",
            "In summary, the `vectorstore_retreiver` uses the `sentence-transformers/all-MiniLM-L6-v2` model to embed the documents and search for similar documents, while the `keyword_retreiver` uses the BM25 algorithm to rank documents based on their keywords. The ensemble retriever combines the results from both retrievers to produce a final ranking of the documents.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"How do the Vector Store vectorstore_retreiver and keyword_retreiver work?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubQmwtOn_S6U"
      },
      "outputs": [],
      "source": [
        "# print(chain.invoke(\"Why is the core temperature in the plasma more difficult to predict?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYvWcyKiNnne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773b0050-12de-4db8-ed7f-8582678c00f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "The chain method in this script is used to create a pipeline of tasks that can be executed in a specific order. The chain is composed of several components:\n",
            "\n",
            "1. `ensemble_retriever`: This is the first component in the chain, which is an `EnsembleRetriever` object that combines the results of two retrievers: `vectorstore_retreiver` and `keyword_retriever`.\n",
            "2. `prompt`: This is a `ChatPromptTemplate` object that defines the prompt for the language model.\n",
            "3. `llm`: This is a `HuggingFaceEndpoint` object that is used to generate text based on the prompt.\n",
            "4. `output_parser`: This is a `StrOutputParser` object that is used to parse the output of the language model into a string.\n",
            "\n",
            "The chain is created by combining these components in a specific order, using the `|` operator. The `|` operator is used to create a pipeline of tasks, where each task is executed in sequence.\n",
            "\n",
            "Here's a breakdown of how the chain works:\n",
            "\n",
            "1. The `ensemble_retriever` component is executed first, which retrieves a list of documents based on the input query.\n",
            "2. The `prompt` component is executed next, which generates a prompt based on the input query.\n",
            "3. The `llm` component is executed next, which generates text based on the prompt.\n",
            "4. The `output_parser` component is executed last, which parses the output of the language model into a string.\n",
            "\n",
            "The chain is then invoked by calling the `invoke` method on the chain object, passing in the input query as an argument. The chain will execute the components in the specified order, and return the output of the last component in the chain.\n",
            "\n",
            "In this specific script, the chain is used to answer a question about the core temperature in a plasma. The input query is passed to the chain, which executes the components in the specified order, and returns the output of the language model as a string. The output is then printed to the console.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"How does the chain method work in this script?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBS4K-h9Nn9X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
